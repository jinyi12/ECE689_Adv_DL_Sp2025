{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y_fk30kCvW9"
      },
      "source": [
        "# ECE 689, Spring 2025\n",
        "## Homework 3\n",
        "\n",
        "## Full name:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl5PDU_JVi-A"
      },
      "source": [
        "## Question 1: Transformer for translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wloM7bzeV1FV"
      },
      "source": [
        "Here, we implement transformers for neural machine translation (NMT), such as turning \"Hello world\" to \"Salut le monde\". You are going to follow the following steps:\n",
        "1. Load and prepare the data. We provide \"en-ft.txt\". Each line of this file contains an English phrase, the equivalent French phrase, and an attribution identifying where the translation came from. The en-fr.txt used in problem 3 can also be found at: https://github.com/jeffprosise/Applied-Machine-Learning/tree/main/Chapter%2013/Data\n",
        "2. Build and train a model. Implement a transformer from scratch in Pytorch. We will provide you with an existing implementation in Keras. You might also find https://github.com/gordicaleksa/pytorch-original-transformer useful.\n",
        "\n",
        "For deliverables, plot your training and validation accuracy. The x-axis should be epoch, the y-axis should be your translation accuracy.\n",
        "\n",
        "For reference, the provided code given at https://github.com/jeffprosise/Applied-Machine-Learning/blob/main/Chapter%2013/Neural%20Machine%20Translation%20(Transformer).ipynb achieves 85% accuracy after 14 epochs. You do not have to achieve the same performance to get full marks, just show understanding and functional codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JI6PUrSdYKs"
      },
      "outputs": [],
      "source": [
        "\"\"\"Clean the text by removing punctuation symbols and numbers, converting\n",
        "characters to lowercase, and replacing Unicode characters with their ASCII\n",
        "equivalents. For the French samples, insert [start] and [end] tokens at the\n",
        " beginning and end of each phrase\"\"\"\n",
        "import pandas as pd\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "\n",
        "df = pd.read_csv('Data/en-fr.txt', names=['en', 'fr', 'attr'], usecols=['en', 'fr'], sep='\\t')\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = normalize('NFD', text.lower())\n",
        "    text = re.sub('[^A-Za-z ]+', '', text)\n",
        "    return text\n",
        "\n",
        "def clean_and_prepare_text(text):\n",
        "    text = '[start] ' + clean_text(text) + ' [end]'\n",
        "    return text\n",
        "\n",
        "df['en'] = df['en'].apply(lambda row: clean_text(row))\n",
        "df['fr'] = df['fr'].apply(lambda row: clean_and_prepare_text(row))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbeUDWOVdftY"
      },
      "outputs": [],
      "source": [
        "\"\"\"The next step is to scan the phrases and determine the maximum length of the\n",
        "English phrases and then of the French phrases. These lengths will determine\n",
        "the lengths of the sequences input to and output from the model\"\"\"\n",
        "en = df['en']\n",
        "fr = df['fr']\n",
        "\n",
        "en_max_len = max(len(line.split()) for line in en)\n",
        "fr_max_len = max(len(line.split()) for line in fr)\n",
        "sequence_len = max(en_max_len, fr_max_len)\n",
        "\n",
        "print(f'Max phrase length (English): {en_max_len}')\n",
        "print(f'Max phrase length (French): {fr_max_len}')\n",
        "print(f'Sequence length: {sequence_len}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8nJpt-OdlRe"
      },
      "outputs": [],
      "source": [
        "\"\"\"Now fit one Tokenizer to the English phrases and another Tokenizer to their\n",
        "French equivalents, and generate padded sequences for all the phrases\"\"\"\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "en_tokenizer = Tokenizer()\n",
        "en_tokenizer.fit_on_texts(en)\n",
        "en_sequences = en_tokenizer.texts_to_sequences(en)\n",
        "en_x = pad_sequences(en_sequences, maxlen=sequence_len, padding='post')\n",
        "\n",
        "fr_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n')\n",
        "fr_tokenizer.fit_on_texts(fr)\n",
        "fr_sequences = fr_tokenizer.texts_to_sequences(fr)\n",
        "fr_y = pad_sequences(fr_sequences, maxlen=sequence_len + 1, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKuUzYdMd0v2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Compute the vocabulary sizes from the Tokenizer instances\"\"\"\n",
        "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
        "\n",
        "print(f'Vocabulary size (English): {en_vocab_size}')\n",
        "print(f'Vocabulary size (French): {fr_vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5UEW_P_d4CF"
      },
      "outputs": [],
      "source": [
        "\"\"\"Finally, create the features and the labels the model will be trained with.\n",
        "The features are the padded English sequences and the padded French sequences\n",
        "minus the [end] tokens. The labels are the padded French sequences minus the\n",
        "[start] tokens. Package the features in a dictionary so they can be input to a\n",
        "model that accepts multiple inputs.\"\"\"\n",
        "inputs = { 'encoder_input': en_x, 'decoder_input': fr_y[:, :-1] }\n",
        "outputs = fr_y[:, 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux5E3cxOefR_"
      },
      "source": [
        "Now, define and train the transformer in Pytorch. We provide here some example code in Keras, **but note that you have to write it in Pytorch**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcut4s6LeyFS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from keras_nlp.layers import TokenAndPositionEmbedding, TransformerEncoder\n",
        "from keras_nlp.layers import TransformerDecoder\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "num_heads = 8\n",
        "embed_dim = 256\n",
        "\n",
        "encoder_input = Input(shape=(None,), dtype='int64', name='encoder_input')\n",
        "x = TokenAndPositionEmbedding(en_vocab_size, sequence_len, embed_dim)(encoder_input)\n",
        "encoder_output = TransformerEncoder(embed_dim, num_heads)(x)\n",
        "encoded_seq_input = Input(shape=(None, embed_dim))\n",
        "\n",
        "decoder_input = Input(shape=(None,), dtype='int64', name='decoder_input')\n",
        "x = TokenAndPositionEmbedding(fr_vocab_size, sequence_len, embed_dim, mask_zero=True)(decoder_input)\n",
        "x = TransformerDecoder(embed_dim, num_heads)(x, encoded_seq_input)\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "decoder_output = Dense(fr_vocab_size, activation='softmax')(x)\n",
        "decoder = Model([decoder_input, encoded_seq_input], decoder_output)\n",
        "decoder_output = decoder([decoder_input, encoder_output])\n",
        "\n",
        "model = Model([encoder_input, decoder_input], decoder_output)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary(line_length=120)\n",
        "\n",
        "callback = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "hist = model.fit(inputs, outputs, epochs=50, validation_split=0.2, callbacks=[callback])\n",
        "\n",
        "acc = hist.history['accuracy']\n",
        "val = hist.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
        "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXBNo7ENgCIR"
      },
      "source": [
        "## Question 2: BERT for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6DNvmkkgRR4"
      },
      "source": [
        "For the last problem, we are going to learn how to use the huggingface library to train a simple BERT classifier for sentiment analysis.\n",
        "\n",
        "We will use the IMDB dataset. You can find the dataset from huggingface using the following command:\n",
        "\n",
        "```\n",
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")\n",
        "```\n",
        "To access BERT, use\n",
        "```\n",
        "from transformers import BertForSequenceClassification\n",
        "#load pre-trained BERT\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                      num_labels = len(label_dict),\n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False)\n",
        "```\n",
        "To reduce training complexity, you can choose to freeze the weight of the pretrained BERT model and only train the classifier. The classifier should have a minimum of 3 layers.\n",
        "You might find https://huggingface.co/blog/sentiment-analysis-python and https://github.com/baotramduong/Twitter-Sentiment-Analysis-with-Deep-Learning-using-BERT/blob/main/Notebook.ipynb helpful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
